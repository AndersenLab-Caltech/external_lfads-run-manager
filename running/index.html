
<!DOCTYPE html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="Documentation of LFADS Run Manager for Matlab">
      
      
      
        <meta name="author" content="Daniel J. O'Shea">
      
      
        <link rel="shortcut icon" href="../assets/images/favicon.png">
      
      <meta name="generator" content="mkdocs-0.16.3, mkdocs-material-1.10.1">
    
    
      
        <title>Running LFADS - LFADS Run Manager</title>
      
    
    
      <script src="../assets/javascripts/modernizr-e826f8942a.js"></script>
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application-a20f419c8e.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-23f75ab9c7.palette.css">
      
    
    
      
        
        
        
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
    
    
    
  </head>
  
  
  
  
    <body data-md-color-primary="red" data-md-color-accent="red">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="drawer">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="search">
    <label class="md-overlay" data-md-component="overlay" for="drawer"></label>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href=".." title="LFADS Run Manager" class="md-header-nav__button md-logo">
          
            <i class="md-icon md-icon--home"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <span class="md-flex__ellipsis md-header-nav__title">
          
            
              
            
            Running LFADS
          
        </span>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="search"></label>
  <div class="md-search__inner">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" required placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query">
      <label class="md-icon md-search__icon" for="search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset">&#xE5CD;</button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result" data-md-lang-search="" data-md-lang-tokenizer="[\s\-]+">
          <div class="md-search-result__meta" data-md-lang-result-none="No matching documents" data-md-lang-result-one="1 matching document" data-md-lang-result-other="# matching documents">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  


  <a href="https://github.com/djoshea/lfads-run-manager" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      djoshea/lfads-run-manager
    </div>
  </a>

          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="drawer">
    <div class="md-nav__button md-logo">
      
        <i class="md-icon md-icon--home"></i>
      
    </div>
    LFADS Run Manager
  </label>
  
    <div class="md-nav__source">
      


  


  <a href="https://github.com/djoshea/lfads-run-manager" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      djoshea/lfads-run-manager
    </div>
  </a>

    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Overview" class="md-nav__link">
      Overview
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../install/" title="Installation" class="md-nav__link">
      Installation
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../matlab/" title="Matlab Classes and Packages" class="md-nav__link">
      Matlab Classes and Packages
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../concepts/" title="Key Concepts" class="md-nav__link">
      Key Concepts
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../adapting/" title="Interfacing with your Datasets" class="md-nav__link">
      Interfacing with your Datasets
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../drive/" title="Preparing for LFADS" class="md-nav__link">
      Preparing for LFADS
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../files/" title="File Organization" class="md-nav__link">
      File Organization
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../multisession/" title="Multisession Stitched Models" class="md-nav__link">
      Multisession Stitched Models
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="toc">
        Running LFADS
      </label>
    
    <a href="./" title="Running LFADS" class="md-nav__link md-nav__link--active">
      Running LFADS
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#launching-each-run-individually-from-shell-scripts" title="Launching each run individually from shell scripts" class="md-nav__link">
    Launching each run individually from shell scripts
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#training-the-model" title="Training the model" class="md-nav__link">
    Training the model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sampling-the-posterior-means" title="Sampling the posterior means" class="md-nav__link">
    Sampling the posterior means
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#writing-the-model-parameters" title="Writing the model parameters" class="md-nav__link">
    Writing the model parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#launching-tensorboard" title="Launching Tensorboard" class="md-nav__link">
    Launching Tensorboard
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lfads-queue-automatically-queueing-many-runs" title="LFADS Queue: Automatically queueing many runs" class="md-nav__link">
    LFADS Queue: Automatically queueing many runs
  </a>
  
</li>
      
      
      
    </ul>
  
</nav>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../analysis/" title="Analyzing LFADS Posterior Means" class="md-nav__link">
      Analyzing LFADS Posterior Means
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../trained_params/" title="Loading the Model Trained Parameters" class="md-nav__link">
      Loading the Model Trained Parameters
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#launching-each-run-individually-from-shell-scripts" title="Launching each run individually from shell scripts" class="md-nav__link">
    Launching each run individually from shell scripts
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#training-the-model" title="Training the model" class="md-nav__link">
    Training the model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sampling-the-posterior-means" title="Sampling the posterior means" class="md-nav__link">
    Sampling the posterior means
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#writing-the-model-parameters" title="Writing the model parameters" class="md-nav__link">
    Writing the model parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#launching-tensorboard" title="Launching Tensorboard" class="md-nav__link">
    Launching Tensorboard
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lfads-queue-automatically-queueing-many-runs" title="LFADS Queue: Automatically queueing many runs" class="md-nav__link">
    LFADS Queue: Automatically queueing many runs
  </a>
  
</li>
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="running-lfads">Running LFADS<a class="headerlink" href="#running-lfads" title="Permanent link">&para;</a></h1>
<p>To train the LFADS model using Python+Tensorflow, you need to generate shell scripts that will actually call <code class="codehilite">run_lfads.py</code> and do the work of training the model. <code class="codehilite">lfads-run-manager</code> provides two ways to go about this.</p>
<div class="admonition warning">
<p class="admonition-title">Add the <code class="codehilite">run_lfads.py</code> folder to your shell PATH</p>
<p>Be sure that the LFADS python source folder is on your shell path, such that running <code class="codehilite">which run_lfads.py</code> prints the directory where the Python+Tensorflow code LFADS is located. If not, you&rsquo;ll need to run something like <code class="codehilite">export PATH=$PATH:/path/to/models/research/lfads</code> and consider adding this to your <code class="codehilite"><span class="na">.bashrc</span></code> file.</p>
<p>If Matlab is able to determine the location of <code class="codehilite">run_lfads.py</code> (meaning that it&rsquo;s own inherited <code class="codehilite">PATH</code> was set correctly), it will prepend an <code class="codehilite">export PATH=...</code> statement to each generated shell script for you. If not, you can try calling <code class="codehilite">setenv(&#39;PATH&#39;, &#39;...&#39;)</code> from within Matlab to add <code class="codehilite">run_lfads.py</code> to the path. before generating the shell scripts.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Virtualenv support</p>
<p>Each of the methods below supports a <code class="codehilite"><span class="s">&#39;virtualenv&#39;</span><span class="p">,</span> <span class="s">&#39;environmentName&#39;</span></code> parameter-value argument. If specified, a <code class="codehilite">source activate environmentName</code> will be prepended to each script that calls Python for you. This is needed when Tensorflow is installed inside a virtual environment.</p>
</div>
<h2 id="launching-each-run-individually-from-shell-scripts">Launching each run individually from shell scripts<a class="headerlink" href="#launching-each-run-individually-from-shell-scripts" title="Permanent link">&para;</a></h2>
<p>It is possible to run each model individually, but you&rsquo;ll probably prefer to <a href="#lfads-queue-automatically-queueing-many-runs">queue everything at once</a>.</p>
<h3 id="training-the-model">Training the model<a class="headerlink" href="#training-the-model" title="Permanent link">&para;</a></h3>
<p>The first is to manually generate shell scripts for each run and then run them yourself. First, for each run <code class="codehilite">i</code>, you will call:</p>
<div class="codehilite"><pre><span></span><span class="n">rc</span><span class="p">.</span><span class="n">runs</span><span class="p">(</span><span class="nb">i</span><span class="p">).</span><span class="n">writeShellScriptLFADSTrain</span><span class="p">(</span><span class="s">&#39;cuda_visible_devices&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s">&#39;display&#39;</span><span class="p">,</span> <span class="mi">500</span><span class="p">);</span>
</pre></div>

<p>Here, you should specify options that will be written into the shell script, the key ones being:</p>
<ul>
<li><code class="codehilite">cuda_visible_devices</code> - which GPU index to run this model on, e.g. <code class="codehilite">0</code>. Use the <code class="codehilite">nvidia-smi</code> to enumerate the available GPUs on your system</li>
<li><code class="codehilite">display</code> - the X display to use, e.g. <code class="codehilite">500</code>. The python code generates plots during training that will appear in TensorBoard. Generating these plots requires a display. When running in a remote server, you&rsquo;ll need to specify this, and possibly to launch an X server using something like <code class="codehilite">tightvnc</code>.</li>
<li><code class="codehilite">appendPosteriorMeanSample</code> - <code class="codehilite">true</code> or <code class="codehilite">false</code> specifying whether to chain the posterior mean sampling operation after the training is finished. The default is <code class="codehilite">false</code>, but if you set this to <code class="codehilite">true</code>, you won&rsquo;t need to call <code class="codehilite">writeShellScriptPosteriorMeanSample</code> below.</li>
<li><code class="codehilite">appendWriteModelParams</code> - <code class="codehilite">true</code> or <code class="codehilite">false</code> specifying whether to chain the posterior mean sampling operation after the training is finished. The default is <code class="codehilite">false</code>, but if you set this to <code class="codehilite">true</code>, you won&rsquo;t need to call <code class="codehilite">writeShellScriptWriteModelParams</code> below.</li>
</ul>
<p>This will generate an <code class="codehilite">lfads_train.sh</code> in the corresponding run&rsquo;s folder. For the first run in our example, this is at
<div class="codehilite"><pre><span></span>~/lorenz_example/runs/exampleRun/param_yMRS4W/single_dataset001/lfads_train.sh
</pre></div></p>
<p>The script essentially launches Python to run <code class="codehilite">run_lfads.py</code> with the specific parameters you&rsquo;ve indicated in <code class="codehilite">RunParams</code> and pointing at the corresponding datasets, which were saved earlier when we called <code class="codehilite">rc.prepareForLFADS</code>.</p>
<div class="codehilite"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span> python <span class="k">$(</span>which run_lfads.py<span class="k">)</span> --data_dir<span class="o">=</span>~/lorenz_example/runs/exampleRun/param_yMRS4W/single_dataset001/lfadsInput --data_filename_stem<span class="o">=</span>lfads --lfads_save_dir<span class="o">=</span>~/lorenz_example/runs/exampleRun/param_yMRS4W/single_dataset001/lfadsOutput --allow_gpu_growth<span class="o">=</span><span class="nb">true</span> --batch_size<span class="o">=</span><span class="m">256</span> --cell_clip_value<span class="o">=</span><span class="m">5</span>.000000 --ci_enc_dim<span class="o">=</span><span class="m">128</span> --co_dim<span class="o">=</span><span class="m">4</span> --con_dim<span class="o">=</span><span class="m">128</span> --controller_input_lag<span class="o">=</span><span class="m">1</span> --device<span class="o">=</span>/gpu:0 --do_causal_controller<span class="o">=</span><span class="nb">false</span> --factors_dim<span class="o">=</span><span class="m">8</span> --gen_dim<span class="o">=</span><span class="m">100</span> --ic_dim<span class="o">=</span><span class="m">64</span> --ic_enc_dim<span class="o">=</span><span class="m">128</span> --in_factors_dim<span class="o">=</span><span class="m">8</span> --keep_prob<span class="o">=</span><span class="m">0</span>.950000 --kl_co_weight<span class="o">=</span><span class="m">1</span>.000000 --kl_ic_weight<span class="o">=</span><span class="m">1</span>.000000 --kl_increase_steps<span class="o">=</span><span class="m">900</span> --l2_con_scale<span class="o">=</span><span class="m">500</span>.000000 --l2_gen_scale<span class="o">=</span><span class="m">500</span>.000000 --l2_increase_steps<span class="o">=</span><span class="m">900</span> --learning_rate_decay_factor<span class="o">=</span><span class="m">0</span>.980000 --learning_rate_stop<span class="o">=</span><span class="m">0</span>.000010 --temporal_spike_jitter_width<span class="o">=</span><span class="m">0</span>
</pre></div>

<p>Running the <code class="codehilite">lfads_train.sh</code> script will launch the Tensorflow training which will take some time. You likely want to launch this in a <code class="codehilite">tmux</code> session if running remotely.</p>
<h3 id="sampling-the-posterior-means">Sampling the posterior means<a class="headerlink" href="#sampling-the-posterior-means" title="Permanent link">&para;</a></h3>
<p>Next, generate the <code class="codehilite">lfads_posterior_mean_sample.sh</code> script to sample the posterior means, which can be launched after training has completed. If you set <code class="codehilite">appendPosteriorMeanSample</code> to <code class="codehilite">true</code> in <code class="codehilite">writeShellScriptLFADSTrain</code>, you can skip this step.</p>
<div class="codehilite"><pre><span></span><span class="n">rc</span><span class="p">.</span><span class="n">runs</span><span class="p">(</span><span class="nb">i</span><span class="p">).</span><span class="n">writeShellScriptLFADSPosteriorMeanSample</span><span class="p">(</span><span class="s">&#39;cuda_visible_devices&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
</pre></div>

<h3 id="writing-the-model-parameters">Writing the model parameters<a class="headerlink" href="#writing-the-model-parameters" title="Permanent link">&para;</a></h3>
<p>Lastly, we want to export the trained model parameters to disk as an HD5 file. We do this by generating the shell script using</p>
<div class="codehilite"><pre><span></span><span class="n">rc</span><span class="p">.</span><span class="n">runs</span><span class="p">(</span><span class="nb">i</span><span class="p">).</span><span class="n">writeShellScriptWriteModelParams</span><span class="p">(</span><span class="s">&#39;cuda_visible_devices&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
</pre></div>

<p>If you set <code class="codehilite">appendWriteModelParams</code> to <code class="codehilite">true</code> in <code class="codehilite">writeShellScriptLFADSTrain</code>, you can skip this step. These results will be written to a file called <code class="codehilite">lfadsOutput/model_params</code>, though these results can be loaded into Matlab using <code class="codehilite">run.loadModelTrainedParams()</code>.</p>
<h3 id="launching-tensorboard">Launching Tensorboard<a class="headerlink" href="#launching-tensorboard" title="Permanent link">&para;</a></h3>
<p>You can monitor the progress of each run by generating a script that launches TensorBoard.
<div class="codehilite"><pre><span></span><span class="n">rc</span><span class="p">.</span><span class="n">writeTensorboardShellScript</span><span class="p">();</span>
</pre></div></p>
<p>This will create <code class="codehilite">launch_tensorboard.sh</code> which will launch Tensorboard which can then be visited at <code class="codehilite">http://localhost:PORT</code>.</p>
<h2 id="lfads-queue-automatically-queueing-many-runs">LFADS Queue: Automatically queueing many runs<a class="headerlink" href="#lfads-queue-automatically-queueing-many-runs" title="Permanent link">&para;</a></h2>
<p>Manually running each of these shell scripts in sequence can be tedious, especially if you don&rsquo;t have enough GPUs or CPUs to run them all in parallel and individual runs take hours or days to complete. To make this part of the process more complete, you can alternatively use the Python task queueing system which will take care of training all the LFADS models for you.</p>
<div class="admonition warning">
<p class="admonition-title">Only supported on Linux</p>
<p>Unfortunately, this task queueing system is not supported on Mac OS at the moment, primarily because it depends on <code class="codehilite">nvidia-smi</code>, though it&rsquo;s theoretically possible with <code class="codehilite">cuda-smi</code> with light code changes. However, Tensorflow has discontinued explicit GPU support on Mac OS anyway. This has also never been tested on Windows, as you&rsquo;d need to get <code class="codehilite">tmux</code> working.</p>
</div>
<p>First, we&rsquo;ll generate the Python script from Matlab that enumerates all of the runs:</p>
<div class="codehilite"><pre><span></span><span class="n">rc</span><span class="p">.</span><span class="n">writeShellScriptRunQueue</span><span class="p">(</span><span class="s">&#39;display&#39;</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="s">&#39;gpuList&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span><span class="p">]);</span>
</pre></div>

<p>The first argument <code class="codehilite">display</code> specifies the X11 display for plotting as before. <code class="codehilite">gpuList</code> enumerates the indices of GPUs that can be used for the runs. This argument is optional if all of the GPUs are viable for Tensorflow on your system.</p>
<div class="admonition note">
<p class="admonition-title">Capping the number of simultaneous runs</p>
<p>You can also manually specify <code class="codehilite">maxTasksSimultaneously</code> if you wish to cap the number of simultaneous runs. By default this is set to the minimum of the number of CPUs on your system and the available GPU memory. By default, each LFADS task is assumed to use 2000 MB of GPU memory, but you can adjust this by specifying <code class="codehilite">gpuMemoryRequired</code>.</p>
</div>
<p>This will generate a Python script <code class="codehilite">run_lfads.py</code>, which for our example lives here:</p>
<div class="codehilite"><pre><span></span>~/lorenz_exajjmple/runs/exampleRun/run_lfadsqueue.py
</pre></div>

<div class="admonition note">
<p class="admonition-title">lfads-run-manager repo folder will be added to your PYTHONPATH automatically</p>
<p>The <code class="codehilite">run_lfadsqueue.py</code> script depends on <code class="codehilite">lfadsqueue.py</code>, which lives in the root of the <code class="codehilite">lfads-run-manager</code> repository. This will be added to your PYTHONPATH environment variable in the <code class="codehilite">run_lfadsqueue.py</code> script.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Install and configure <code class="codehilite">tmux</code></p>
<p>The LFADS queue launches each LFADS run inside its own <code class="codehilite">tmux</code> session to make it easy to monitor the runs as they are running. You&rsquo;ll need to install <code class="codehilite">tmux</code>.</p>
<p>Also, <code class="codehilite">tmux</code> is finnicky about environment variables, which are only loaded when the <code class="codehilite">tmux</code> server first launches, not when a new session is started. The main one you need is that <code class="codehilite">run_lfads.py</code> must be on your <code class="codehilite">PATH</code> somewhere. If Matlab is able to determine this location (meaning that it&rsquo;s own inherited <code class="codehilite">PATH</code> was set correctly), it will prepend an <code class="codehilite">export PATH=...</code> statement to each <code class="codehilite">lfads_train.sh</code> script for you. If not, you can try calling <code class="codehilite">setenv(&#39;PATH&#39;, &#39;...&#39;)</code> from within Matlab to add <code class="codehilite">run_lfads.py</code> to the path. before generating the shell scripts.</p>
<p>If you&rsquo;re having trouble, you might want to launch a new <code class="codehilite">tmux</code> session using:</p>
<div class="codehilite"><pre><span></span>tmux new-session
</pre></div>

<p>Then from inside <code class="codehilite">tmux</code>, test that <code class="codehilite">which run_lfads.py</code> prints a location and that you are able to launch python and run <code class="codehilite"><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span></code> without any issues.</p>
</div>
<p>You can then kick everything off by running <code class="codehilite">python run_lfadsqueue.py</code> at the command line. It&rsquo;s recommended to do this from inside your own <code class="codehilite">tmux</code> session if you&rsquo;re running on a remote server, so you can monitor the task runner.</p>
<div class="admonition tip">
<p class="admonition-title">Python virtual environments</p>
<p>If tensorflow is installed in a Python virtual environment, you can have this environment be automatically <code class="codehilite">source activate</code>d within the training scripts using:
<div class="codehilite"><pre><span></span><span class="n">rc</span><span class="p">.</span><span class="n">writeShellScriptRunQueue</span><span class="p">(</span><span class="s">&#39;display&#39;</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="s">&#39;gpuList&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span><span class="p">],</span> <span class="s">&#39;virtualenv&#39;</span><span class="p">,</span> <span class="s">&#39;tensorflow&#39;</span><span class="p">);</span>
</pre></div></p>
</div>
<p>A few notes on how the system works:</p>
<ul>
<li>Output from Python will be <code class="codehilite">tee</code>&lsquo;d into <code class="codehilite">lfads.out</code>, so you can check the output during or afterwards either there or in the <code class="codehilite">tmux</code> session.</li>
<li>When a model finishes training and posterior mean sampling, a file called <code class="codehilite">lfads.done</code> will be created</li>
<li>If the task runner detects an <code class="codehilite">lfads.done</code> file, it will skip that run. Unless you pass <code class="codehilite"><span class="s">&#39;rerun&#39;</span><span class="p">,</span> <span class="n">true</span></code> to <code class="codehilite">writeShellScriptRunQueue</code>, in which case every run will be rerun. This is convenient if you&rsquo;ve added additional runs and just want the new ones to run.</li>
<li>If a run fails, the error will be printed by the task runner and <code class="codehilite">lfads.done</code> will not be created</li>
<li>A running tally of how many runs are currently running, have finished, or have failed will be printed</li>
<li>You can enter a run&rsquo;s <code class="codehilite">tmux</code> session directly to monitor it. The list of sessions can be obtained using <code class="codehilite">tmux list-sessions</code>. You can also abort it using <code class="codehilite">Ctrl-C</code> and it will be marked as failed by the task runner.</li>
</ul>
<p>The <code class="codehilite">run_lfadsqueue.py</code> script will periodically output updates about how the runs are proceeding:</p>
<div class="codehilite"><pre><span></span><span class="o">(</span>tensorflow<span class="o">)</span> ➜  exampleRun python run_lfadsqueue.py
Warning: tmux sessions will be nested inside the current session
Queue: Launching TensorBoard on port <span class="m">42561</span> in tmux session exampleRun_tensorboard_port42561
bash /home/djoshea/lorenz_example/runs/exampleRun/launch_tensorboard.sh --port<span class="o">=</span><span class="m">42561</span>
Queue: Initializing with <span class="m">2</span> GPUs and <span class="m">12</span> CPUs, max <span class="m">4</span> simultaneous tasks
Task lfads_param_pqQbzB__single_dataset001: launching on gpu <span class="m">0</span>
Task lfads_param_pqQbzB__single_dataset001: started in tmux session lfads_param_pqQbzB__single_dataset001 on GPU <span class="m">0</span> with PID <span class="m">19498</span>
Task lfads_param_pqQbzB__single_dataset002: launching on gpu <span class="m">1</span>
Task lfads_param_pqQbzB__single_dataset002: started in tmux session lfads_param_pqQbzB__single_dataset002 on GPU <span class="m">1</span> with PID <span class="m">19527</span>
Task lfads_param_pqQbzB__single_dataset003: launching on gpu <span class="m">0</span>
Task lfads_param_pqQbzB__single_dataset003: started in tmux session lfads_param_pqQbzB__single_dataset003 on GPU <span class="m">0</span> with PID <span class="m">19551</span>
Task lfads_param_pqQbzB__all: launching on gpu <span class="m">1</span>
Task lfads_param_pqQbzB__all: started in tmux session lfads_param_pqQbzB__all on GPU <span class="m">1</span> with PID <span class="m">19585</span>
Task lfads_param_pqQbzB__single_dataset003:      Decreasing learning rate to <span class="m">0</span>.009800.
Task lfads_param_pqQbzB__single_dataset001:      Decreasing learning rate to <span class="m">0</span>.009800.
Task lfads_param_pqQbzB__single_dataset001:      Decreasing learning rate to <span class="m">0</span>.009604.
Task lfads_param_pqQbzB__single_dataset003:      Decreasing learning rate to <span class="m">0</span>.009604.
Task lfads_param_pqQbzB__single_dataset003:      Decreasing learning rate to <span class="m">0</span>.009412.
Task lfads_param_pqQbzB__single_dataset001:      Decreasing learning rate to <span class="m">0</span>.009412.
</pre></div>

<p>As the tasks run, the task queue will print out messages related to decreasing the learning rate, which is one way to measure ongonig progress towards the termination criterion (when the learning rate hits <code class="codehilite">c_learning_rate_stop</code>). When a task fails or completes, the queue will print out a running tally.</p>
<p>Note that TensorBoard has automatically been launched on an available port, here on <code class="codehilite">42561</code>. You can also directly attach to the tmux sessions whose names are indicated in the script as &ldquo;Tasks&rdquo;, which can be listed using <code class="codehilite">tmux list-sessions</code>.</p>
<div class="codehilite"><pre><span></span>➜  exampleRun tmux list-sessions
matlab: <span class="m">4</span> windows <span class="o">(</span>created Tue Oct  <span class="m">3</span> <span class="m">21</span>:51:49 <span class="m">2017</span><span class="o">)</span> <span class="o">[</span>201x114<span class="o">]</span> <span class="o">(</span>attached<span class="o">)</span>
exampleRun_tensorboard_port42561: <span class="m">1</span> windows <span class="o">(</span>created Fri Oct  <span class="m">6</span> <span class="m">14</span>:43:16 <span class="m">2017</span><span class="o">)</span> <span class="o">[</span>201x113<span class="o">]</span>
lfads_param_pqQbzB__all: <span class="m">1</span> windows <span class="o">(</span>created Fri Oct  <span class="m">6</span> <span class="m">14</span>:43:17 <span class="m">2017</span><span class="o">)</span> <span class="o">[</span>201x113<span class="o">]</span>
lfads_param_pqQbzB__single_dataset001: <span class="m">1</span> windows <span class="o">(</span>created Fri Oct  <span class="m">6</span> <span class="m">14</span>:43:16 <span class="m">2017</span><span class="o">)</span> <span class="o">[</span>201x114<span class="o">]</span>
lfads_param_pqQbzB__single_dataset002: <span class="m">1</span> windows <span class="o">(</span>created Fri Oct  <span class="m">6</span> <span class="m">14</span>:43:16 <span class="m">2017</span><span class="o">)</span> <span class="o">[</span>201x113<span class="o">]</span>
lfads_param_pqQbzB__single_dataset003: <span class="m">1</span> windows <span class="o">(</span>created Fri Oct  <span class="m">6</span> <span class="m">14</span>:43:17 <span class="m">2017</span><span class="o">)</span> <span class="o">[</span>201x113<span class="o">]</span>
</pre></div>

<p>If you wish to abort ongoing runs, you can either attach to them directly and use <code class="codehilite">Ctrl-C</code>, or use <code class="codehilite">tmux kill-session SESSIONNAME</code>. When everything has completed, you&rsquo;ll see something like this:</p>
<div class="codehilite"><pre><span></span>Task lfads_param_pqQbzB__all: Stopping optimization based on learning rate criteria.
Task lfads_param_pqQbzB__all: completed successfully
Queue: All tasks completed.
Queue: <span class="m">0</span> skipped, <span class="m">4</span> finished, <span class="m">0</span> failed, <span class="m">0</span> running
</pre></div>
                
                  
                
              
              
                
              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../multisession/" title="Multisession Stitched Models" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Multisession Stitched Models
              </span>
            </div>
          </a>
        
        
          <a href="../analysis/" title="Analyzing LFADS Posterior Means" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Analyzing LFADS Posterior Means
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2017 &mdash; Daniel J. O&#39;Shea
          </div>
        
        powered by
        <a href="http://www.mkdocs.org" title="MkDocs">MkDocs</a>
        and
        <a href="http://squidfunk.github.io/mkdocs-material/" title="Material for MkDocs">
          Material for MkDocs</a>
      </div>
      
        
  <div class="md-footer-social">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    
      <a href="https://github.com/djoshea" class="md-footer-social__link fa fa-github"></a>
    
      <a href="https://twitter.com/djoshea" class="md-footer-social__link fa fa-twitter"></a>
    
  </div>

      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application-f3ab9e5ff8.js"></script>
      
      
      <script>app.initialize({url:{base:".."}})</script>
      
    
    
      
      <script>!function(e,t,a,n,o,c,i){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,c=t.createElement(a),i=t.getElementsByTagName(a)[0],c.async=1,c.src=n,i.parentNode.insertBefore(c,i)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-322693-7","auto"),ga("set","anonymizeIp",!0),ga("send","pageview");var links=document.getElementsByTagName("a");Array.prototype.map.call(links,function(e){e.host!=document.location.host&&e.addEventListener("click",function(){var t=e.getAttribute("data-md-action")||"follow";ga("send","event","outbound",t,e.href)})});var query=document.forms.search.query;query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})</script>
      
    
  </body>
</html>